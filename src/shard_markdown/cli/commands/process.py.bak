"""Process command for document processing."""

from pathlib import Path

import click
from rich.console import Console
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)
from rich.table import Table

from ...chromadb.collections import CollectionManager
from ...chromadb.factory import create_chromadb_client
from ...chromadb.protocol import ChromaDBClientProtocol
from ...core.models import BatchResult, ChunkingConfig, ProcessingResult
from ...core.processor import DocumentProcessor
from ...types import (
    ChromaCollectionProtocol,
    ConfigType,
    ProgressProtocol,
)
from ...utils.errors import ShardMarkdownError
from ...utils.logging import get_logger
from ...utils.validation import (
    validate_chunk_parameters,
    validate_collection_name,
    validate_input_paths,
)


logger = get_logger(__name__)
console = Console()


@click.command()
@click.argument("input_paths", nargs=-1, required=True, type=click.Path(exists=True))
@click.option(
    "--collection", "-c", required=True, help="Target ChromaDB collection name"
)
@click.option(
    "--chunk-size",
    "-s",
    default=1000,
    type=int,
    help="Maximum chunk size in characters [default: 1000]",
)
@click.option(
    "--chunk-overlap",
    "-o",
    default=200,
    type=int,
    help="Overlap between chunks in characters [default: 200]",
)
@click.option(
    "--chunk-method",
    type=click.Choice(["structure", "fixed"]),
    default="structure",
    help="Chunking method [default: structure]",
)
@click.option("--recursive", "-r", is_flag=True, help="Process directories recursively")
@click.option(
    "--create-collection",
    is_flag=True,
    help="Create collection if it doesn't exist",
)
@click.option(
    "--clear-collection",
    is_flag=True,
    help="Clear existing collection before processing",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Show what would be processed without executing",
)
@click.option(
    "--collection-metadata",
    help="Additional metadata for new collections (JSON format)",
)
@click.pass_context
def process(  # noqa: C901
    ctx: click.Context,
    input_paths: tuple,
    collection: str,
    chunk_size: int,
    chunk_overlap: int,
    chunk_method: str,
    recursive: bool,
    create_collection: bool,
    clear_collection: bool,
    dry_run: bool,
    collection_metadata: str | None,
) -> None:
    """Process markdown files into ChromaDB collections.

    This command processes one or more markdown files, intelligently chunks
    them based on document structure, and stores the results in a ChromaDB
    collection.

    Examples:
      # Process a single file
      shard-md process --collection my-docs document.md

      # Process multiple files with custom chunking
      shard-md process -c tech-docs --chunk-size 1500 --chunk-overlap 300 *.md

      # Process directory recursively
      shard-md process -c all-docs --recursive ./docs/

      # Create new collection and clear it first
      shard-md process -c new-docs --create-collection --clear-collection *.md
    """
    # Handle cases where ctx.obj might be None (e.g., in tests)
    if ctx.obj is None:
        ctx.obj = {}

    config = ctx.obj.get("config")
    if config is None:
        # Create a default config for testing
        from ...config.settings import AppConfig

        config = AppConfig()

    verbose = ctx.obj.get("verbose", 0)

    try:
        # Validate and prepare
        validated_paths = _validate_and_prepare(
            input_paths, chunk_size, chunk_overlap, collection, recursive
        )
        if dry_run:
            _show_dry_run_preview(
                validated_paths, collection, chunk_size, chunk_overlap
            )
            return

        console.print(
            f"[blue]Processing {len(validated_paths)} markdown files...[/blue]"
        )

        # Setup ChromaDB and collection
        chroma_client, collection_obj = _setup_chromadb_and_collection(
            config, collection, clear_collection, create_collection
        )

        # Initialize processor
        processor = DocumentProcessor(
            ChunkingConfig(
                chunk_size=chunk_size, overlap=chunk_overlap, method=chunk_method
            )
        )

        # Process files
        _process_files_with_progress(
            validated_paths,
            collection,
            processor,
            chroma_client,
            collection_obj,
            verbose,
        )

    except ShardMarkdownError as e:
        _handle_shard_error(e, verbose)
    except Exception as e:
        _handle_unexpected_error(e, verbose)


def _validate_and_prepare(
    input_paths: tuple,
    chunk_size: int,
    chunk_overlap: int,
    collection: str,
    recursive: bool,
) -> list[Path]:
    """Validate parameters and prepare input paths."""
    validate_chunk_parameters(chunk_size, chunk_overlap)
    validate_collection_name(collection)
    return validate_input_paths(list(input_paths), recursive)


def _setup_chromadb_and_collection(
    config: ConfigType,
    collection: str,
    clear_collection: bool,
    create_collection: bool,
) -> tuple[ChromaDBClientProtocol, ChromaCollectionProtocol]:
    """Set up ChromaDB client and collection."""
    from ...utils.errors import ChromaDBError, NetworkError

    chroma_client = create_chromadb_client(config.chromadb)
    try:
        if not chroma_client.connect():
            raise click.ClickException("Failed to connect to ChromaDB")
    except (NetworkError, ChromaDBError) as e:
        # Re-raise with the specific error message from the exception
        raise click.ClickException(str(e)) from e

    collection_manager = CollectionManager(chroma_client)

    # Handle collection clearing
    if clear_collection:
        _handle_collection_clearing(chroma_client, collection_manager, collection)

    # Get or create collection
    try:
        collection_obj = collection_manager.get_or_create_collection(
            collection, create_if_missing=create_collection
        )
    except AttributeError:
        # Fallback for mock objects or older implementations
        collection_obj = chroma_client.get_or_create_collection(
            collection, create_if_missing=create_collection
        )

    return chroma_client, collection_obj


def _handle_collection_clearing(
    chroma_client: ChromaDBClientProtocol,
    collection_manager: CollectionManager,
    collection: str,
) -> None:
    """Handle collection clearing if requested."""
    try:
        collection_manager.delete_collection(collection)
        console.print(f"[yellow]Cleared collection '{collection}'[/yellow]")
    except (ValueError, RuntimeError):
        # Collection doesn't exist or can't be deleted
        pass


def _process_files_with_progress(
    validated_paths: list[Path],
    collection: str,
    processor: DocumentProcessor,
    chroma_client: ChromaDBClientProtocol,
    collection_obj: ChromaCollectionProtocol,
    verbose: int,
) -> None:
    """Process files with progress display."""
    total_files = len(validated_paths)

    if total_files == 1:
        _process_single_file(
            validated_paths[0], collection, processor, chroma_client, collection_obj
        )
    else:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("Processing files...", total=total_files)
            _process_batch_files(
                validated_paths,
                collection,
                processor,
                chroma_client,
                collection_obj,
                verbose,
                progress,
            )


def _process_single_file(
    file_path: Path,
    collection: str,
    processor: DocumentProcessor,
    chroma_client: ChromaDBClientProtocol,
    collection_obj: ChromaCollectionProtocol,
) -> None:
    """Process a single file."""
    try:
        # Process the document
        processing_result = processor.process_file(file_path)

        # Insert into ChromaDB
        insert_result = chroma_client.bulk_insert(
            collection_obj, processing_result.chunks
        )

        # Display results
        _display_single_result(processing_result, insert_result)

    except Exception as e:
        console.print(f"[red]Error processing {file_path}: {str(e)}[/red]")
        logger.error("Failed to process file %s: %s", file_path, str(e))


def _process_batch_files(
    validated_paths: list[Path],
    collection: str,
    processor: DocumentProcessor,
    chroma_client: ChromaDBClientProtocol,
    collection_obj: ChromaCollectionProtocol,
    verbose: int,
    progress: ProgressProtocol,
) -> None:
    """Process multiple files in batch."""
    batch_result = processor.process_batch(validated_paths, collection)

    # Bulk insert all chunks
    all_chunks = []
    for result in batch_result.results:
        if result.success and hasattr(result, "chunks"):
            all_chunks.extend(result.chunks)

    if all_chunks:
        insert_result = chroma_client.bulk_insert(collection_obj, all_chunks)
    else:
        from ...core.models import InsertResult

        insert_result = InsertResult(
            success=False,
            chunks_inserted=0,
            error="No chunks to insert",
            collection_name=collection,
        )

    # Display results
    _display_batch_results(batch_result, insert_result, verbose)


def _handle_shard_error(e: ShardMarkdownError, verbose: int) -> None:
    """Handle ShardMarkdownError exceptions."""
    console.print(f"[red]Error:[/red] {e.message}")
    if verbose > 0:
        console.print(f"[dim]Error code: {e.error_code}[/dim]")
        if e.context:
            console.print(f"[dim]Context: {e.context}[/dim]")
    raise click.Abort()


def _handle_unexpected_error(e: Exception, verbose: int) -> None:
    """Handle unexpected exceptions."""
    console.print(f"[red]Unexpected error:[/red] {str(e)}")
    if verbose > 0:
        import traceback

        console.print(f"[dim]{traceback.format_exc()}[/dim]")
    raise click.Abort() from e


def _show_dry_run_preview(
    validated_paths: list[Path], collection: str, chunk_size: int, chunk_overlap: int
) -> None:
    """Show dry run preview."""
    table = Table(title="Dry Run Preview")
    table.add_column("File", style="cyan")
    table.add_column("Size", style="white")
    table.add_column("Collection", style="green")

    for path in validated_paths:
        file_size = path.stat().st_size
        table.add_row(str(path), f"{file_size:,} bytes", collection)

    console.print(table)
    console.print("\n[yellow]Configuration:[/yellow]")
    console.print(f"  Chunk size: {chunk_size}")
    console.print(f"  Chunk overlap: {chunk_overlap}")
    console.print(f"  Target collection: {collection}")


def _display_single_result(
    processing_result: ProcessingResult, insert_result: object
) -> None:
    """Display results for single file processing."""
    if processing_result.success:
        console.print(
            f"[green]✓[/green] Processed [cyan]{processing_result.file_path}[/cyan]"
        )
        console.print(f"  Created {processing_result.chunks_created} chunks")
        console.print(f"  Processing time: {processing_result.processing_time:.2f}s")

        if hasattr(insert_result, "success") and insert_result.success:
            chunks_inserted = getattr(insert_result, "chunks_inserted", 0)
            console.print(f"  Inserted {chunks_inserted} chunks into ChromaDB")
        else:
            console.print("[yellow]  Warning: Failed to insert chunks[/yellow]")
    else:
        console.print(
            f"[red]✗[/red] Failed to process [cyan]{processing_result.file_path}[/cyan]"
        )
        if processing_result.error:
            console.print(f"  Error: {processing_result.error}")


def _display_batch_results(
    batch_result: BatchResult, insert_result: object, verbose: int
) -> None:
    """Display results for batch processing."""
    console.print("\n[green]Batch Processing Complete[/green]")
    console.print(f"Files processed: {batch_result.total_files}")
    console.print(f"Successful: {batch_result.successful_files}")
    console.print(f"Failed: {batch_result.failed_files}")
    console.print(f"Success rate: {batch_result.success_rate:.1f}%")
    console.print(f"Total chunks created: {batch_result.total_chunks}")
    console.print(f"Processing time: {batch_result.total_processing_time:.2f}s")

    if hasattr(insert_result, "success") and insert_result.success:
        chunks_inserted = getattr(insert_result, "chunks_inserted", 0)
        console.print(f"Chunks inserted: {chunks_inserted}")
    else:
        console.print("[yellow]Warning: Failed to insert some/all chunks[/yellow]")

    if verbose > 0:
        console.print(
            f"Average chunks per file: {batch_result.average_chunks_per_file:.1f}"
        )
        console.print(
            f"Processing speed: {batch_result.processing_speed:.2f} files/sec"
        )
