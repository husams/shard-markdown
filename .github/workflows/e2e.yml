name: End-to-End Tests

# This workflow provides comprehensive E2E testing with real ChromaDB:
# - CLI workflow testing with ChromaDB v1.0.16+
# - Package installation testing (wheel, source, editable)
# - ChromaDB configuration and compatibility testing
# - Performance and load testing (scheduled runs)
# All tests use real ChromaDB instances, not mocks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run E2E tests every day at 2 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.12"

jobs:
  # Test CLI workflows with real ChromaDB
  e2e-cli:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest] # Only test on Ubuntu, Windows not needed
        # ChromaDB 1.0.16+ is required for this tool
        # 1.0.16: Minimum supported version with v1/v2 API support
        chromadb-version: ["1.0.16"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: |
          # Retry logic for Python installation to handle transient network issues
          for i in {1..3}; do
            if uv python install ${{ env.PYTHON_VERSION }}; then
              echo "Python installation successful"
              break
            else
              echo "Attempt $i failed, retrying in 10 seconds..."
              sleep 10
            fi
            if [ $i -eq 3 ]; then
              echo "Python installation failed after 3 attempts"
              exit 1
            fi
          done

      - name: Install dependencies
        run: uv sync --dev --extra chromadb

      - name: Setup ChromaDB with version-aware detection
        uses: ./.github/actions/setup-chromadb
        with:
          chromadb-version: ${{ matrix.chromadb-version }}
          port: 8000
          timeout: 120

      - name: Create test markdown files
        run: |
          mkdir -p test-docs

          echo "# Test Document 1" > test-docs/doc1.md
          echo "" >> test-docs/doc1.md
          echo "This is a test document for the CLI." >> test-docs/doc1.md
          echo "" >> test-docs/doc1.md
          echo "## Section A" >> test-docs/doc1.md
          echo "Some content in section A." >> test-docs/doc1.md
          echo "" >> test-docs/doc1.md
          echo "## Section B" >> test-docs/doc1.md
          echo "Some content in section B." >> test-docs/doc1.md

          echo "---" > test-docs/doc2.md
          echo "title: Test Document 2" >> test-docs/doc2.md
          echo "author: Test Author" >> test-docs/doc2.md
          echo "tags: [test, markdown]" >> test-docs/doc2.md
          echo "---" >> test-docs/doc2.md
          echo "" >> test-docs/doc2.md
          echo "# Another Test Document" >> test-docs/doc2.md
          echo "" >> test-docs/doc2.md
          echo "This document has frontmatter." >> test-docs/doc2.md

      - name: Test CLI help commands
        run: |
          uv run shard-md --help
          uv run shard-md process --help
          uv run shard-md collections --help
          uv run shard-md query --help
          uv run shard-md config --help

      - name: Verify ChromaDB connectivity
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          echo "Testing ChromaDB connectivity..."
          curl -f http://localhost:8000/api/v1/heartbeat || curl -f http://localhost:8000/api/v2/heartbeat || curl -f http://localhost:8000/heartbeat || {
            echo "ChromaDB not responding on port 8000"
            exit 1
          }
          echo "ChromaDB is accessible"

          # Test with Python to ensure client can connect
          uv run python -c "import os; os.environ['CHROMA_HOST'] = 'localhost'; os.environ['CHROMA_PORT'] = '8000'; from shard_markdown.chromadb.client import ChromaDBClient; from shard_markdown.config.settings import ChromaDBConfig; config = ChromaDBConfig(host='localhost', port=8000); client = ChromaDBClient(config); print('âœ… Python client can connect to ChromaDB') if client.connect() else (print('âŒ Python client cannot connect to ChromaDB'), exit(1))"

      - name: Test configuration commands
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Initialize configuration with explicit ChromaDB settings
          echo "Creating config with ChromaDB connection settings..."
          cat > shard-md-config.yaml << EOF
          chromadb:
            host: localhost
            port: 8000
            timeout: 30

          chunking:
            default_size: 1000
            default_overlap: 200
            method: structure

          processing:
            batch_size: 10
          EOF

          echo "Config file created:"
          cat shard-md-config.yaml

          # Show configuration
          uv run shard-md --config shard-md-config.yaml config show

      - name: Test collection management
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
          SHARD_MD_USE_MOCK_CHROMADB: "false" # Ensure we're not using mock
        run: |
          # Debug: Show environment and check Python imports
          echo "Environment variables:"
          echo "CHROMA_HOST=$CHROMA_HOST"
          echo "CHROMA_PORT=$CHROMA_PORT"

          # Test if chromadb is importable
          uv run python -c "import chromadb; print(f'ChromaDB version: {chromadb.__version__}')" || echo "ChromaDB import failed"

          # Test direct connection with Python
          uv run python -c "import chromadb; client = chromadb.HttpClient(host='localhost', port=8000); print(f'Direct ChromaDB connection: {client.heartbeat()}')" || echo "Direct connection failed"

          # Create collection with verbose output and explicit config
          uv run shard-md --config shard-md-config.yaml -vvv collections create test-collection --description "Test collection for E2E" || {
            echo "Collection create failed. Showing debug info..."
            echo "Current directory: $(pwd)"
            echo "Config file exists: $(ls -la shard-md-config.yaml 2>/dev/null || echo 'NO')"
            echo "Environment: CHROMA_HOST=$CHROMA_HOST CHROMA_PORT=$CHROMA_PORT"
            
            # Try with mock as fallback to ensure CLI works
            echo "Trying with mock client..."
            SHARD_MD_USE_MOCK_CHROMADB=true uv run shard-md collections create test-collection-mock --description "Mock test" || echo "Mock also failed"
            exit 1
          }

          # List collections
          uv run shard-md --config shard-md-config.yaml collections list

          # Get collection info
          uv run shard-md --config shard-md-config.yaml collections info test-collection

      - name: Run pytest E2E tests
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
          SHARD_MD_USE_MOCK_CHROMADB: "false" # Ensure we're using real ChromaDB
        run: |
          echo "ðŸ§ª Running pytest E2E tests with real ChromaDB"
          uv run pytest tests/e2e/ -v --tb=short --strict-markers
          echo "âœ… All E2E tests passed"

      - name: Test document processing
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Process single document
          uv run shard-md --config shard-md-config.yaml process --collection test-collection test-docs/doc1.md

          # Process multiple documents
          uv run shard-md --config shard-md-config.yaml process --collection test-collection --recursive test-docs/

          # Dry run
          uv run shard-md --config shard-md-config.yaml process --collection test-collection --dry-run test-docs/doc2.md

      - name: Test document querying
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Search documents
          uv run shard-md --config shard-md-config.yaml query search "test document" --collection test-collection --limit 5

          # Verify documents in collection
          DOC_COUNT=$(uv run shard-md --config shard-md-config.yaml collections info test-collection --format json | python -c "import sys, json; data=json.load(sys.stdin); print(data.get('count', 0))")
          echo "Collection has $DOC_COUNT documents"

      - name: Test error handling
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Test with non-existent collection
          if uv run shard-md --config shard-md-config.yaml query search "test" --collection non-existent 2>&1 | grep -q "error\|Error"; then
            echo "âœ… Error handling works for non-existent collection"
          else
            echo "âŒ Error handling failed for non-existent collection"
            exit 1
          fi

          # Test with non-existent file
          if uv run shard-md --config shard-md-config.yaml process --collection test-collection non-existent.md 2>&1 | grep -q "error\|Error"; then
            echo "âœ… Error handling works for non-existent file"
          else
            echo "âŒ Error handling failed for non-existent file"
            exit 1
          fi

      - name: Cleanup test collection
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          uv run shard-md --config shard-md-config.yaml collections delete test-collection || true

  # Test package installation from different sources
  e2e-installation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: |
          # Retry logic for Python installation to handle transient network issues
          for i in {1..3}; do
            if uv python install ${{ env.PYTHON_VERSION }}; then
              echo "Python installation successful"
              break
            else
              echo "Attempt $i failed, retrying in 10 seconds..."
              sleep 10
            fi
            if [ $i -eq 3 ]; then
              echo "Python installation failed after 3 attempts"
              exit 1
            fi
          done

      - name: Build package
        run: |
          uv sync --dev
          uv build

      - name: Test wheel installation
        run: |
          # Create fresh environment
          python -m venv test-wheel-env
          source test-wheel-env/bin/activate

          # Install from wheel
          pip install dist/*.whl

          # Test basic functionality
          shard-md --help
          shard-md --version

          deactivate
          rm -rf test-wheel-env

      - name: Test source installation
        run: |
          # Create fresh environment
          python -m venv test-source-env
          source test-source-env/bin/activate

          # Install from source
          pip install dist/*.tar.gz

          # Test basic functionality
          shard-md --help
          shard-md --version

          deactivate
          rm -rf test-source-env

      - name: Test editable installation
        run: |
          # Create fresh environment
          python -m venv test-editable-env
          source test-editable-env/bin/activate

          # Install in editable mode
          pip install -e .

          # Test basic functionality
          shard-md --help
          shard-md --version

          deactivate
          rm -rf test-editable-env

  # Performance and load testing with stable versions
  e2e-performance:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: |
          # Retry logic for Python installation to handle transient network issues
          for i in {1..3}; do
            if uv python install ${{ env.PYTHON_VERSION }}; then
              echo "Python installation successful"
              break
            else
              echo "Attempt $i failed, retrying in 10 seconds..."
              sleep 10
            fi
            if [ $i -eq 3 ]; then
              echo "Python installation failed after 3 attempts"
              exit 1
            fi
          done

      - name: Install dependencies
        run: uv sync --dev --extra chromadb

      - name: Setup ChromaDB for performance testing
        uses: ./.github/actions/setup-chromadb
        with:
          chromadb-version: "1.0.16" # Use latest stable for performance testing
          port: 8000
          timeout: 120

      - name: Generate large test dataset
        run: |
          mkdir -p large-test-docs

          for i in {1..50}; do
            echo "---" > large-test-docs/doc$i.md
            echo "title: Large Document $i" >> large-test-docs/doc$i.md
            echo "category: performance-test" >> large-test-docs/doc$i.md
            echo "---" >> large-test-docs/doc$i.md
            echo "" >> large-test-docs/doc$i.md
            echo "# Large Document $i" >> large-test-docs/doc$i.md
            echo "" >> large-test-docs/doc$i.md

            for j in {1..20}; do
              echo "## Section $j" >> large-test-docs/doc$i.md
              echo "" >> large-test-docs/doc$i.md
              echo "This is section $j of document $i. It contains some sample text for performance testing." >> large-test-docs/doc$i.md
              echo "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua." >> large-test-docs/doc$i.md
              echo "" >> large-test-docs/doc$i.md
            done
          done

      - name: Performance test - Batch processing
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Create config for performance test
          cat > perf-test-config.yaml << EOF
          chromadb:
            host: localhost
            port: 8000
          EOF

          uv run shard-md --config perf-test-config.yaml collections create performance-test

          time uv run shard-md --config perf-test-config.yaml process --collection performance-test --recursive large-test-docs/

          # Verify all documents were processed
          DOC_COUNT=$(uv run shard-md --config perf-test-config.yaml collections info performance-test --format json | python -c "import sys, json; data=json.load(sys.stdin); print(data.get('count', 0))")
          echo "Processed $DOC_COUNT document chunks"

          if [ "$DOC_COUNT" -lt 100 ]; then
            echo "âŒ Performance test failed: Too few documents processed"
            exit 1
          fi

      - name: Performance test - Search queries
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Test search performance
          time uv run shard-md --config perf-test-config.yaml query search "Lorem ipsum" --collection performance-test --limit 10
          time uv run shard-md --config perf-test-config.yaml query search "Section" --collection performance-test --limit 20
          time uv run shard-md --config perf-test-config.yaml query search "Document" --collection performance-test --limit 5

      - name: Cleanup performance test
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          uv run shard-md --config perf-test-config.yaml collections delete performance-test || true

  # Note: e2e-chromadb-configs and e2e-version-compatibility jobs removed to reduce duplication
  # All ChromaDB testing is now consolidated in the main e2e-cli job with version 1.0.16
