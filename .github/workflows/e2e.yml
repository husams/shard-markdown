name: End-to-End Tests

# This workflow provides comprehensive E2E testing with real ChromaDB:
# - CLI workflow testing with ChromaDB v1.0.16+
# - Package installation testing (wheel, source, editable)
# - ChromaDB configuration and compatibility testing
# - Performance and load testing (scheduled runs)
# All tests use real ChromaDB instances, not mocks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run E2E tests every day at 2 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.12"

jobs:
  # Test CLI workflows with real ChromaDB
  e2e-cli:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest] # Only test on Ubuntu, Windows not needed
        # ChromaDB 1.0.16+ is required for this tool
        # 1.0.16: Minimum supported version with v1/v2 API support
        chromadb-version: ["1.0.16"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --dev --extra chromadb

      - name: Setup ChromaDB with version-aware detection
        uses: ./.github/actions/setup-chromadb
        with:
          chromadb-version: ${{ matrix.chromadb-version }}
          port: 8000
          timeout: 120

      - name: Create test markdown files
        run: |
          mkdir -p test-docs

          echo "# Test Document 1" > test-docs/doc1.md
          echo "" >> test-docs/doc1.md
          echo "This is a test document for the CLI." >> test-docs/doc1.md
          echo "" >> test-docs/doc1.md
          echo "## Section A" >> test-docs/doc1.md
          echo "Some content in section A." >> test-docs/doc1.md
          echo "" >> test-docs/doc1.md
          echo "## Section B" >> test-docs/doc1.md
          echo "Some content in section B." >> test-docs/doc1.md

          echo "---" > test-docs/doc2.md
          echo "title: Test Document 2" >> test-docs/doc2.md
          echo "author: Test Author" >> test-docs/doc2.md
          echo "tags: [test, markdown]" >> test-docs/doc2.md
          echo "---" >> test-docs/doc2.md
          echo "" >> test-docs/doc2.md
          echo "# Another Test Document" >> test-docs/doc2.md
          echo "" >> test-docs/doc2.md
          echo "This document has frontmatter." >> test-docs/doc2.md

      - name: Test CLI help commands
        run: |
          uv run shard-md --help
          uv run shard-md --version

      - name: Verify ChromaDB connectivity
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          echo "Testing ChromaDB connectivity..."
          curl -f http://localhost:8000/api/v1/heartbeat || curl -f http://localhost:8000/api/v2/heartbeat || curl -f http://localhost:8000/heartbeat || {
            echo "ChromaDB not responding on port 8000"
            exit 1
          }
          echo "ChromaDB is accessible"

          # Test with Python to ensure client can connect
          uv run python -c "import os; os.environ['CHROMA_HOST'] = 'localhost'; os.environ['CHROMA_PORT'] = '8000'; from shard_markdown.chromadb.client import ChromaDBClient; from shard_markdown.config.settings import Settings; config = Settings(chroma_host='localhost', chroma_port=8000); client = ChromaDBClient(config); print('âœ… Python client can connect to ChromaDB') if client.connect() else (print('âŒ Python client cannot connect to ChromaDB'), exit(1))"

      - name: Test configuration loading
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Create configuration file
          echo "Creating config file..."
          cat > shard-md.yaml << EOF
          chroma_host: localhost
          chroma_port: 8000
          chunk_size: 1000
          chunk_overlap: 200
          chunk_method: structure
          EOF

          echo "Config file created:"
          cat shard-md.yaml

          # Test that config is loaded
          uv run shard-md test-docs/doc1.md --config-path shard-md.yaml --dry-run

      - name: Test ChromaDB connectivity
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
          SHARD_MD_USE_MOCK_CHROMADB: "false" # Ensure we're not using mock
        run: |
          # Debug: Show environment and check Python imports
          echo "Environment variables:"
          echo "CHROMA_HOST=$CHROMA_HOST"
          echo "CHROMA_PORT=$CHROMA_PORT"

          # Test if chromadb is importable
          uv run python -c "import chromadb; print(f'ChromaDB version: {chromadb.__version__}')" || echo "ChromaDB import failed"

          # Test direct connection with Python
          uv run python -c "import chromadb; client = chromadb.HttpClient(host='localhost', port=8000); print(f'Direct ChromaDB connection: {client.heartbeat()}')" || echo "Direct connection failed"

          # Test that storage works by processing a file
          echo "# Test Storage" > test-storage.md
          echo "Testing ChromaDB storage functionality" >> test-storage.md

          # Store document in ChromaDB
          uv run shard-md test-storage.md --store --collection test-collection --config-path shard-md.yaml || {
            echo "Storage failed. Showing debug info..."
            echo "Current directory: $(pwd)"
            echo "Config file exists: $(ls -la shard-md.yaml 2>/dev/null || echo 'NO')"
            echo "Environment: CHROMA_HOST=$CHROMA_HOST CHROMA_PORT=$CHROMA_PORT"
            exit 1
          }

      - name: Run pytest E2E tests
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
          SHARD_MD_USE_MOCK_CHROMADB: "false" # Ensure we're using real ChromaDB
        run: |
          echo "ðŸ§ª Running pytest E2E tests with real ChromaDB"
          uv run pytest tests/e2e/ -v --tb=short --strict-markers
          echo "âœ… All E2E tests passed"

      - name: Test document processing
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Process single document with storage
          uv run shard-md test-docs/doc1.md --store --collection test-collection --config-path shard-md.yaml

          # Process directory with storage
          uv run shard-md test-docs/ --store --collection test-collection --recursive --config-path shard-md.yaml

          # Dry run (display mode)
          uv run shard-md test-docs/doc2.md --dry-run --config-path shard-md.yaml

      - name: Test document processing results
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Test that documents were stored by processing more files
          echo "# Verification Document" > verify.md
          echo "This document verifies storage is working" >> verify.md

          # Store another document to verify collection exists
          uv run shard-md verify.md --store --collection test-collection --config-path shard-md.yaml

          echo "âœ… Document storage verification completed"

      - name: Test error handling
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Test with non-existent file
          if uv run shard-md non-existent.md 2>&1 | grep -q "error\|Error\|not found\|does not exist"; then
            echo "âœ… Error handling works for non-existent file"
          else
            echo "âŒ Error handling failed for non-existent file"
            exit 1
          fi

          # Test with invalid options
          if uv run shard-md test-docs/doc1.md --store 2>&1 | grep -q "collection is required"; then
            echo "âœ… Error handling works for missing collection"
          else
            echo "âŒ Error handling failed for missing collection"
            exit 1
          fi

      - name: Cleanup test files
        run: |
          rm -f test-storage.md verify.md shard-md.yaml
          rm -rf test-docs/
          echo "âœ… Test files cleaned up"

  # Test package installation from different sources
  e2e-installation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Build package
        run: |
          uv sync --dev
          uv build

      - name: Test wheel installation
        run: |
          # Create fresh environment
          python -m venv test-wheel-env
          source test-wheel-env/bin/activate

          # Install from wheel
          pip install dist/*.whl

          # Test basic functionality
          shard-md --help
          shard-md --version

          deactivate
          rm -rf test-wheel-env

      - name: Test source installation
        run: |
          # Create fresh environment
          python -m venv test-source-env
          source test-source-env/bin/activate

          # Install from source
          pip install dist/*.tar.gz

          # Test basic functionality
          shard-md --help
          shard-md --version

          deactivate
          rm -rf test-source-env

      - name: Test editable installation
        run: |
          # Create fresh environment
          python -m venv test-editable-env
          source test-editable-env/bin/activate

          # Install in editable mode
          pip install -e .

          # Test basic functionality
          shard-md --help
          shard-md --version

          deactivate
          rm -rf test-editable-env

  # Performance and load testing with stable versions
  e2e-performance:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --dev --extra chromadb

      - name: Setup ChromaDB for performance testing
        uses: ./.github/actions/setup-chromadb
        with:
          chromadb-version: "1.0.16" # Use latest stable for performance testing
          port: 8000
          timeout: 120

      - name: Generate large test dataset
        run: |
          mkdir -p large-test-docs

          for i in {1..50}; do
            echo "---" > large-test-docs/doc$i.md
            echo "title: Large Document $i" >> large-test-docs/doc$i.md
            echo "category: performance-test" >> large-test-docs/doc$i.md
            echo "---" >> large-test-docs/doc$i.md
            echo "" >> large-test-docs/doc$i.md
            echo "# Large Document $i" >> large-test-docs/doc$i.md
            echo "" >> large-test-docs/doc$i.md

            for j in {1..20}; do
              echo "## Section $j" >> large-test-docs/doc$i.md
              echo "" >> large-test-docs/doc$i.md
              echo "This is section $j of document $i. It contains some sample text for performance testing." >> large-test-docs/doc$i.md
              echo "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua." >> large-test-docs/doc$i.md
              echo "" >> large-test-docs/doc$i.md
            done
          done

      - name: Performance test - Batch processing
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Create config for performance test
          cat > perf-test-config.yaml << EOF
          chroma_host: localhost
          chroma_port: 8000
          chunk_size: 500
          chunk_overlap: 50
          EOF

          # Process all documents with storage
          time uv run shard-md large-test-docs/ --store --collection performance-test --recursive --config-path perf-test-config.yaml

          echo "âœ… Performance test documents processed"

      - name: Performance test - Verify storage
        env:
          CHROMA_HOST: localhost
          CHROMA_PORT: 8000
        run: |
          # Verify by storing additional documents
          echo "# Performance Verification" > perf-verify.md
          echo "Verifying performance test collection" >> perf-verify.md

          time uv run shard-md perf-verify.md --store --collection performance-test --config-path perf-test-config.yaml

          echo "âœ… Performance test verification completed"

      - name: Cleanup performance test
        run: |
          rm -rf large-test-docs/
          rm -f perf-test-config.yaml perf-verify.md
          echo "âœ… Performance test cleanup completed"

  # Note: e2e-chromadb-configs and e2e-version-compatibility jobs removed to reduce duplication
  # All ChromaDB testing is now consolidated in the main e2e-cli job with version 1.0.16
